{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extracting filenames\n",
    "path = '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample'\n",
    "filenames = []\n",
    "    \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for directory in sorted(dirs):\n",
    "        #print directory[-2:]  #corpus\n",
    "        for r, d, f in os.walk(path+'/'+directory):\n",
    "            for name in sorted(f):\n",
    "                filenames.append(path+'/'+directory+'/'+name) #populating filenames array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.02253446  0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.01986979 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "10\n",
      "16469\n"
     ]
    }
   ],
   "source": [
    "#char 8-gram tfidf feature\n",
    "tfidf = TfidfVectorizer(input='filename',use_idf=True,analyzer='char',ngram_range=(8,8))\n",
    "eight_char = tfidf.fit_transform(filenames).toarray()\n",
    "print eight_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8         0.          0.025       0.          0.          0.2       ]\n",
      " [ 0.86206897  0.          0.10344828  0.          0.          0.27586207]\n",
      " [ 2.          0.          0.23076923  0.07692308  0.07692308  0.84615385]\n",
      " [ 1.34615385  0.          0.03846154  0.          0.          0.30769231]\n",
      " [ 0.5         0.10714286  0.17857143  0.          0.          0.67857143]\n",
      " [ 0.66666667  0.          0.07692308  0.          0.          0.15384615]\n",
      " [ 1.04545455  0.09090909  0.          0.          0.          0.81818182]\n",
      " [ 1.05882353  0.35294118  0.11764706  0.          0.          0.52941176]\n",
      " [ 0.53521127  0.          0.          0.          0.          0.11267606]\n",
      " [ 1.03846154  0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#punctiation frequence of (',' ':' ';' '(' ')' '!') in text (normaized by thenumber of sentences)\n",
    "def punctuation(text):\n",
    "    sentences = text.replace('\\n','')\n",
    "    sentences = sentences.replace('.','\\n').splitlines()\n",
    "    matrix = np.zeros((1, 6))\n",
    "    matrix[0, 0] = text.count(',')/float(len(sentences))\n",
    "    matrix[0, 1] = text.count(':')/float(len(sentences))\n",
    "    matrix[0, 2] = text.count(';')/float(len(sentences))\n",
    "    matrix[0, 3] = text.count('(')/float(len(sentences))\n",
    "    matrix[0, 4] = text.count(')')/float(len(sentences))\n",
    "    matrix[0, 5] = text.count('!')/float(len(sentences))\n",
    "    return matrix\n",
    "\n",
    "train = \"\"\n",
    "i = 0\n",
    "for doc in filenames:\n",
    "    f = open(doc,\"r\")\n",
    "    train = f.read()\n",
    "    tmp = punctuation(train)\n",
    "    if i == 0: \n",
    "            punct = tmp\n",
    "            i += 1\n",
    "    else: \n",
    "        punct = np.concatenate((punct,tmp))\n",
    "print punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4770318 ]\n",
      " [ 0.51456311]\n",
      " [ 0.50340136]\n",
      " [ 0.49693252]\n",
      " [ 0.50153846]\n",
      " [ 0.50416667]\n",
      " [ 0.57053292]\n",
      " [ 0.52805281]\n",
      " [ 0.41538462]\n",
      " [ 0.55033557]]\n"
     ]
    }
   ],
   "source": [
    "#words number of stop words in text / number of total words in text\n",
    "8char = tfidf.fit_transform(filenames).toarray()\n",
    "print 8char\n",
    "def vocabulary(text):\n",
    "    count = CountVectorizer(analyzer='word',ngram_range=(1,1),stop_words='english')\n",
    "    countTotal = CountVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    counter = count.fit_transform([text]).toarray()\n",
    "    countT = countTotal.fit_transform([text]).toarray()\n",
    "    matrix = np.zeros((1, 1))\n",
    "    matrix[0, 0] = (countT.sum()-counter.sum())/float(countT.sum())\n",
    "\n",
    "    return matrix\n",
    "\n",
    "train = \"\"\n",
    "i = 0\n",
    "for doc in filenames:\n",
    "    f = open(doc,\"r\")\n",
    "    train = f.read()\n",
    "    tmp = vocabulary(train)\n",
    "    if i == 0: \n",
    "            vocab = tmp\n",
    "            i += 1\n",
    "    else: \n",
    "        vocab = np.concatenate((vocab,tmp))\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   4.38748219    6.5           1.           18.           40.        ]\n",
      " [   7.66262452   10.20689655    2.           30.           29.        ]\n",
      " [  30.79863979   23.53846154    1.          109.           13.        ]\n",
      " [   7.71573356   12.92307692    4.           30.           26.        ]\n",
      " [   9.91159906   10.78571429    1.           37.           28.        ]\n",
      " [  13.36800226   12.25641026    1.           84.           39.        ]\n",
      " [   8.83655537   14.22727273    3.           30.           22.        ]\n",
      " [  11.48792054   18.29411765    3.           53.           17.        ]\n",
      " [   4.26102314    5.11267606    0.           24.           71.        ]\n",
      " [   7.5499324    10.80769231    1.           27.           26.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Average and standard deviation of words per sentence\n",
    "def phrases(text):\n",
    "    sentences = text.replace('\\n','')\n",
    "    sentences = sentences.replace('.','\\n').splitlines()\n",
    "    wps = np.array([len(s.split()) for s in sentences]) #wps contains the number of words in each sentence\n",
    "    matrix = np.zeros((1, 5))\n",
    "    matrix[0, 0] = wps.std() #deviation\n",
    "    matrix[0, 1] = wps.mean() #mean\n",
    "    matrix[0, 2] = np.amin(wps) #min\n",
    "    matrix[0, 3] = np.amax(wps) #max\n",
    "    matrix[0, 4] = len(sentences) #number of sentences\n",
    "    return matrix\n",
    "\n",
    "train = \"\"\n",
    "i = 0\n",
    "for doc in filenames:\n",
    "    f = open(doc,\"r\")\n",
    "    train = f.read()\n",
    "    tmp = phrases(train)\n",
    "    if i == 0: \n",
    "            phrase = tmp\n",
    "            i += 1\n",
    "    else: \n",
    "        phrase = np.concatenate((phrase,tmp))\n",
    "print phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert text to tags using TreeTagger wrapper for Python\n",
    "def text_to_tags(text):\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='en',TAGDIR='/home/yassine/EMSE 2015-2016/Projet Recherche/tree-tagger-linux-3.2')\n",
    "    tags = treetaggerwrapper.make_tags(tagger.tag_text(unicode(text,encoding='utf-8')))\n",
    "    pos_tags = []\n",
    "    for pos in tags:\n",
    "        pos_tags.append(pos[1])\n",
    "    return \" \".join(pos_tags)\n",
    "\n",
    "#print text_to_tags('Hello world, My name is Yassine better, I had taken taking best better !') #example\n",
    "\n",
    "\n",
    "#write documents as tags to file (RUNS ONCE!)\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for directory in sorted(dirs):\n",
    "        for r, d, f in os.walk(path+'/'+directory):\n",
    "            for name in sorted(f):\n",
    "                f = open(path+'/'+directory+'/'+name,\"r\")\n",
    "                train = f.read()\n",
    "                tags = text_to_tags(train)\n",
    "                name = re.sub('\\.txt$', '', name)\n",
    "                f_tags = open(path+'/'+directory+'/'+name + '_tags.txt',\"w\")\n",
    "                f_tags.write(tags)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "33\n",
      "30\n",
      "31\n",
      "32\n",
      "35\n",
      "32\n",
      "27\n",
      "32\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "#contructing pos frequency and 4-pos grams\n",
    "def four_pos(text):\n",
    "    tags = text_to_tags(text)\n",
    "    countTotal = CountVectorizer(analyzer='word',ngram_range=(1,1),token_pattern=r\"\\b\\w+\\b\")\n",
    "    transformer = TfidfTransformer()\n",
    "    countT = countTotal.fit_transform([tags]).toarray()\n",
    "    mat_pos = transformer.fit_transform(countT).toarray()\n",
    "    return mat_pos\n",
    "\n",
    "train = \"\"\n",
    "i = 0\n",
    "tmp = []\n",
    "for doc in filenames:\n",
    "    f = open(doc,\"r\")\n",
    "    train = f.read()\n",
    "    print len(four_pos(train)[0])\n",
    "    tmp.append(four_pos(train)[0])\n",
    "#pos_grams = np.array(tmp)\n",
    "#print len(pos_grams)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
