{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import treetaggerwrapper\n",
    "import time\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN001/known01.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN001/unknown.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN002/known01.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN002/unknown.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN003/known01.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN003/unknown.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN004/known01.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN004/unknown.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN005/known01.txt', '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample/EN005/unknown.txt']\n"
     ]
    }
   ],
   "source": [
    "#extracting filenames\n",
    "path = '/home/yassine/EMSE 2015-2016/Projet Recherche/Author-Verification-/corpus-english-sample'\n",
    "filenames = []\n",
    "    \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for directory in sorted(dirs):\n",
    "        #print directory[-2:]  #corpus\n",
    "        for r, d, f in os.walk(path+'/'+directory):\n",
    "            for name in sorted(f):\n",
    "                if  not \"tags\" in name and not \"first\" in name and not \"last\" in name:\n",
    "                    filenames.append(path+'/'+directory+'/'+name) #populating filenames array\n",
    "print filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.0324937   0.00551451  0.00251454  0.00751858  0.00391661\n",
      "   0.00833561  0.00616206  0.00729071  0.00247608]\n",
      " [ 0.0324937   1.          0.00295753  0.00759594  0.01129755  0.0051768\n",
      "   0.01301554  0.00389864  0.01950875  0.01015311]\n",
      " [ 0.00551451  0.00295753  1.          0.0076365   0.00375494  0.00246882\n",
      "   0.00582625  0.01264807  0.00505329  0.00416161]\n",
      " [ 0.00251454  0.00759594  0.0076365   1.          0.00458907  0.00446063\n",
      "   0.00321113  0.01242658  0.00362829  0.00584841]\n",
      " [ 0.00751858  0.01129755  0.00375494  0.00458907  1.          0.00725262\n",
      "   0.00766288  0.00480944  0.00201179  0.00514724]\n",
      " [ 0.00391661  0.0051768   0.00246882  0.00446063  0.00725262  1.\n",
      "   0.00667673  0.00866406  0.01091604  0.00481545]\n",
      " [ 0.00833561  0.01301554  0.00582625  0.00321113  0.00766288  0.00667673\n",
      "   1.          0.00411899  0.00689463  0.0050672 ]\n",
      " [ 0.00616206  0.00389864  0.01264807  0.01242658  0.00480944  0.00866406\n",
      "   0.00411899  1.          0.00316951  0.00281593]\n",
      " [ 0.00729071  0.01950875  0.00505329  0.00362829  0.00201179  0.01091604\n",
      "   0.00689463  0.00316951  1.          0.0052316 ]\n",
      " [ 0.00247608  0.01015311  0.00416161  0.00584841  0.00514724  0.00481545\n",
      "   0.0050672   0.00281593  0.0052316   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#char 8-gram tfidf feature\n",
    "tfidf = TfidfVectorizer(input='filename',use_idf=True,analyzer='char',ngram_range=(8,8))\n",
    "eight_char = tfidf.fit_transform(filenames).toarray() #matrice of nbr of doc * nbr of n-gram with 1 <= n <= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8         0.          0.025       0.          0.          0.2       ]\n",
      " [ 0.86206897  0.          0.10344828  0.          0.          0.27586207]\n",
      " [ 2.          0.          0.23076923  0.07692308  0.07692308  0.84615385]\n",
      " [ 1.34615385  0.          0.03846154  0.          0.          0.30769231]\n",
      " [ 0.5         0.10714286  0.17857143  0.          0.          0.67857143]\n",
      " [ 0.66666667  0.          0.07692308  0.          0.          0.15384615]\n",
      " [ 1.04545455  0.09090909  0.          0.          0.          0.81818182]\n",
      " [ 1.05882353  0.35294118  0.11764706  0.          0.          0.52941176]\n",
      " [ 0.53521127  0.          0.          0.          0.          0.11267606]\n",
      " [ 1.03846154  0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#punctiation frequence of (',' ':' ';' '(' ')' '!') in text (normaized by thenumber of sentences)\n",
    "def punctuation(text):\n",
    "    sentences = text.replace('\\n','')\n",
    "    sentences = sentences.replace('.','\\n').splitlines()\n",
    "    matrix = np.zeros((1, 6))\n",
    "    matrix[0, 0] = text.count(',')/float(len(sentences))\n",
    "    matrix[0, 1] = text.count(':')/float(len(sentences))\n",
    "    matrix[0, 2] = text.count(';')/float(len(sentences))\n",
    "    matrix[0, 3] = text.count('(')/float(len(sentences))\n",
    "    matrix[0, 4] = text.count(')')/float(len(sentences))\n",
    "    matrix[0, 5] = text.count('!')/float(len(sentences))\n",
    "    return matrix\n",
    "\n",
    "train = \"\"\n",
    "i = 0\n",
    "for doc in filenames:\n",
    "    f = open(doc,\"r\")\n",
    "    train = f.read()\n",
    "    tmp = punctuation(train)\n",
    "    if i == 0: \n",
    "            punct = tmp\n",
    "            i += 1\n",
    "    else: \n",
    "        punct = np.concatenate((punct,tmp))\n",
    "print punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4770318 ]\n",
      " [ 0.51456311]\n",
      " [ 0.50340136]\n",
      " [ 0.49693252]\n",
      " [ 0.50153846]\n",
      " [ 0.50416667]\n",
      " [ 0.57053292]\n",
      " [ 0.52805281]\n",
      " [ 0.41538462]\n",
      " [ 0.55033557]]\n"
     ]
    }
   ],
   "source": [
    "#words number of stop words in text / number of total words in text\n",
    "def vocabulary(text):\n",
    "    count = CountVectorizer(analyzer='word',ngram_range=(1,1),stop_words='english')\n",
    "    countTotal = CountVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    counter = count.fit_transform([text]).toarray()\n",
    "    countT = countTotal.fit_transform([text]).toarray()\n",
    "    matrix = np.zeros((1, 1))\n",
    "    matrix[0, 0] = (countT.sum()-counter.sum())/float(countT.sum())\n",
    "\n",
    "    return matrix\n",
    "\n",
    "train = \"\"\n",
    "i = 0\n",
    "for doc in filenames:\n",
    "    f = open(doc,\"r\")\n",
    "    train = f.read()\n",
    "    tmp = vocabulary(train)\n",
    "    if i == 0: \n",
    "            vocab = tmp\n",
    "            i += 1\n",
    "    else: \n",
    "        vocab = np.concatenate((vocab,tmp))\n",
    "        \n",
    "print vocab\n",
    "\n",
    "dist = DistanceMetric.get_metric('euclidean')\n",
    "vocabM = dist.pairwise(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   4.38748219    6.5           1.           18.           40.        ]\n",
      " [   7.66262452   10.20689655    2.           30.           29.        ]\n",
      " [  30.79863979   23.53846154    1.          109.           13.        ]\n",
      " [   7.71573356   12.92307692    4.           30.           26.        ]\n",
      " [   9.91159906   10.78571429    1.           37.           28.        ]\n",
      " [  13.36800226   12.25641026    1.           84.           39.        ]\n",
      " [   8.83655537   14.22727273    3.           30.           22.        ]\n",
      " [  11.48792054   18.29411765    3.           53.           17.        ]\n",
      " [   4.26102314    5.11267606    0.           24.           71.        ]\n",
      " [   7.5499324    10.80769231    1.           27.           26.        ]]\n",
      "10\n",
      "5\n",
      "[[ 1.          0.85451514  0.10797081  0.79469995  0.73691478  0.52958702\n",
      "   0.68349222  0.3043982   0.99462024  0.84405191]\n",
      " [ 0.85451514  1.          0.6047468   0.99085421  0.97967547  0.88664996\n",
      "   0.95659697  0.75398495  0.80072214  0.99690152]\n",
      " [ 0.10797081  0.6047468   1.          0.66696218  0.75076331  0.89159888\n",
      "   0.76885388  0.9639353   0.01763809  0.61135878]\n",
      " [ 0.79469995  0.99085421  0.66696218  1.          0.9852508   0.9152761\n",
      "   0.9829304   0.81780676  0.73085569  0.99173387]\n",
      " [ 0.73691478  0.97967547  0.75076331  0.9852508   1.          0.95584206\n",
      "   0.98277092  0.86187219  0.67030797  0.97915067]\n",
      " [ 0.52958702  0.88664996  0.89159888  0.9152761   0.95584206  1.\n",
      "   0.94240577  0.95137663  0.45352886  0.87958654]\n",
      " [ 0.68349222  0.95659697  0.76885388  0.9829304   0.98277092  0.94240577\n",
      "   1.          0.89578668  0.6061983   0.96672469]\n",
      " [ 0.3043982   0.75398495  0.9639353   0.81780676  0.86187219  0.95137663\n",
      "   0.89578668  1.          0.21100935  0.76296914]\n",
      " [ 0.99462024  0.80072214  0.01763809  0.73085569  0.67030797  0.45352886\n",
      "   0.6061983   0.21100935  1.          0.78632165]\n",
      " [ 0.84405191  0.99690152  0.61135878  0.99173387  0.97915067  0.87958654\n",
      "   0.96672469  0.76296914  0.78632165  1.        ]]\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Average and standard deviation of words per sentence\n",
    "def phrases(text):\n",
    "    sentences = text.replace('\\n','')\n",
    "    sentences = sentences.replace('.','\\n').splitlines()\n",
    "    wps = np.array([len(s.split()) for s in sentences]) #wps contains the number of words in each sentence\n",
    "    matrix = np.zeros((1, 5))\n",
    "    matrix[0, 0] = wps.std() #deviation\n",
    "    matrix[0, 1] = wps.mean() #mean\n",
    "    matrix[0, 2] = np.amin(wps) #min\n",
    "    matrix[0, 3] = np.amax(wps) #max\n",
    "    matrix[0, 4] = len(sentences) #number of sentences\n",
    "    return matrix\n",
    "\n",
    "train = \"\"\n",
    "i = 0\n",
    "for doc in filenames:\n",
    "    f = open(doc,\"r\")\n",
    "    train = f.read()\n",
    "    tmp = phrases(train)\n",
    "    if i == 0: \n",
    "            phrase = tmp\n",
    "            i += 1\n",
    "    else: \n",
    "        phrase = np.concatenate((phrase,tmp))\n",
    "print phrase #matrice of nbr of doc * 5 \n",
    "\n",
    "phraseM = np.corrcoef(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treating file: EN001/known01.txt...\n",
      "0.003002 seconds\n",
      "writing: EN001/known01_tags.txt...\n",
      "51.695993 seconds\n",
      "writing: EN001/known01_first.txt...\n",
      "51.696350 seconds\n",
      "writing: EN001/known01_last.txt...\n",
      "51.696530 seconds\n",
      "treating file: EN001/unknown.txt...\n",
      "51.696758 seconds\n",
      "writing: EN001/unknown_tags.txt...\n",
      "87.561970 seconds\n",
      "writing: EN001/unknown_first.txt...\n",
      "87.562606 seconds\n",
      "writing: EN001/unknown_last.txt...\n",
      "87.562795 seconds\n",
      "treating file: EN002/known01.txt...\n",
      "87.563155 seconds\n",
      "writing: EN002/known01_tags.txt...\n",
      "116.340978 seconds\n",
      "writing: EN002/known01_first.txt...\n",
      "116.341606 seconds\n",
      "writing: EN002/known01_last.txt...\n",
      "116.341846 seconds\n",
      "treating file: EN002/unknown.txt...\n",
      "116.342053 seconds\n",
      "writing: EN002/unknown_tags.txt...\n",
      "169.334862 seconds\n",
      "writing: EN002/unknown_first.txt...\n",
      "169.335474 seconds\n",
      "writing: EN002/unknown_last.txt...\n",
      "169.335658 seconds\n",
      "treating file: EN003/known01.txt...\n",
      "169.336014 seconds\n",
      "writing: EN003/known01_tags.txt...\n",
      "219.744309 seconds\n",
      "writing: EN003/known01_first.txt...\n",
      "219.744677 seconds\n",
      "writing: EN003/known01_last.txt...\n",
      "219.744856 seconds\n",
      "treating file: EN003/unknown.txt...\n",
      "219.745080 seconds\n",
      "writing: EN003/unknown_tags.txt...\n",
      "263.442842 seconds\n",
      "writing: EN003/unknown_first.txt...\n",
      "263.443313 seconds\n",
      "writing: EN003/unknown_last.txt...\n",
      "263.443482 seconds\n",
      "treating file: EN004/known01.txt...\n",
      "263.443882 seconds\n",
      "writing: EN004/known01_tags.txt...\n",
      "305.665156 seconds\n",
      "writing: EN004/known01_first.txt...\n",
      "305.665555 seconds\n",
      "writing: EN004/known01_last.txt...\n",
      "305.665820 seconds\n",
      "treating file: EN004/unknown.txt...\n",
      "305.666062 seconds\n",
      "writing: EN004/unknown_tags.txt...\n",
      "339.359285 seconds\n",
      "writing: EN004/unknown_first.txt...\n",
      "339.359666 seconds\n",
      "writing: EN004/unknown_last.txt...\n",
      "339.359829 seconds\n",
      "treating file: EN005/known01.txt...\n",
      "339.360188 seconds\n",
      "writing: EN005/known01_tags.txt...\n",
      "416.182705 seconds\n",
      "writing: EN005/known01_first.txt...\n",
      "416.183342 seconds\n",
      "writing: EN005/known01_last.txt...\n",
      "416.183519 seconds\n",
      "treating file: EN005/unknown.txt...\n",
      "416.183716 seconds\n",
      "writing: EN005/unknown_tags.txt...\n",
      "450.827470 seconds\n",
      "writing: EN005/unknown_first.txt...\n",
      "450.828075 seconds\n",
      "writing: EN005/unknown_last.txt...\n",
      "450.828232 seconds\n"
     ]
    }
   ],
   "source": [
    "#convert text to tags using TreeTagger wrapper for Python\n",
    "def text_to_tags(text):\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='en',TAGDIR='/home/yassine/EMSE 2015-2016/Projet Recherche/tree-tagger-linux-3.2')\n",
    "    tags = treetaggerwrapper.make_tags(tagger.tag_text(unicode(text,encoding='utf-8')))\n",
    "    pos_tags = []\n",
    "    for pos in tags:\n",
    "        pos_tags.append(pos[1])\n",
    "    return \" \".join(pos_tags)\n",
    "\n",
    "#print text_to_tags('Hello world, My name is Yassine better, I had taken taking best better !') #example\n",
    "\n",
    "filenames_tags = []\n",
    "filenames_first = []\n",
    "filenames_last = []\n",
    "\n",
    "start_time = time.time()\n",
    "#write documents as tags to file \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for directory in sorted(dirs):\n",
    "        for r, d, f in os.walk(path+'/'+directory):\n",
    "            for name in sorted(f):\n",
    "                first_tags = []\n",
    "                last_tags = []\n",
    "                text_tags = []\n",
    "                if not \"tags\" in name and not \"first\" in name and not \"last\" in name:\n",
    "                    f = open(path+'/'+directory+'/'+name,\"r\")\n",
    "                    train = f.read()\n",
    "                    print 'treating file: '+ directory +'/'+ name + '...' \n",
    "                    print \"%f seconds\" % (time.time() - start_time)\n",
    "                    #converting text to pos tags sentence by sentence to keep first and last tags seperately\n",
    "                    sentences = train.replace('\\n','')\n",
    "                    sentences = train.replace('.','\\n').splitlines()\n",
    "                    for s in sentences:\n",
    "                        s = text_to_tags(s)\n",
    "                        first_tags.append(\" \".join(s.split()[0:1])) #add first pos tags of each sentence to 'first_tags'\n",
    "                        last_tags.append(\" \".join(s.split()[-1:]))  #add last pos tags of each sentence to 'last_tags'\n",
    "                        text_tags.append(s)       #add all pos tags of each sentence to 'lext_tags'\n",
    "                    \n",
    "                    print 'writing: '+directory+'/'+re.sub('\\.txt$', '', name)+'_tags.txt...'\n",
    "                    print \"%f seconds\" % (time.time() - start_time)\n",
    "                    f_tags = open(path+'/'+directory+'/'+ re.sub('\\.txt$', '', name)+'_tags.txt',\"w\") #write entire text in tags to file\n",
    "                    f_tags.write(\" \".join(text_tags))\n",
    "                    f_tags.close()   \n",
    "                    filenames_tags.append(path+'/'+directory+'/'+ re.sub('\\.txt$', '', name)+'_tags.txt') \n",
    "                    \n",
    "                    print 'writing: ' + directory+'/' +re.sub('\\.txt$', '', name)+'_first.txt...'\n",
    "                    print \"%f seconds\" % (time.time() - start_time)\n",
    "                    f_first = open(path+'/'+directory+'/'+ re.sub('\\.txt$', '', name)+'_first.txt',\"w\") #write first tags to file\n",
    "                    f_first.write(\" \".join(first_tags))\n",
    "                    f_first.close() \n",
    "                    filenames_first.append(path+'/'+directory+'/'+ re.sub('\\.txt$', '', name)+'_first.txt') \n",
    "                    \n",
    "                    print 'writing: '+ directory+'/' +re.sub('\\.txt$', '', name)+'_last.txt...'\n",
    "                    print \"%f seconds\" % (time.time() - start_time)\n",
    "                    f_last = open(path+'/'+directory+'/'+ re.sub('\\.txt$', '', name)+'_last.txt',\"w\") #write last tags to file\n",
    "                    f_last.write(\" \".join(last_tags))\n",
    "                    f_last.close() \n",
    "                    filenames_last.append(path+'/'+directory+'/'+ re.sub('\\.txt$', '', name)+'_last.txt') \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.04637389 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.04319342  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.04993762 ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.04828045 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#contructing pos frequency and 4-pos grams\n",
    "freq_four_pos = TfidfVectorizer(input='filename',use_idf=False,analyzer='word',ngram_range=(4,4))\n",
    "four_pos = freq_four_pos.fit_transform(filenames_tags).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "36\n",
      "[[ 1.          0.86948548  0.6903805   0.80016954  0.8537235   0.76899953\n",
      "   0.73475192  0.69995341  0.81531999  0.86029135]\n",
      " [ 0.86948548  1.          0.68043349  0.68768926  0.6288413   0.61707149\n",
      "   0.61339686  0.54154676  0.63344707  0.73973355]\n",
      " [ 0.6903805   0.68043349  1.          0.76872284  0.72058348  0.70288038\n",
      "   0.77112718  0.65006077  0.64736141  0.79694558]\n",
      " [ 0.80016954  0.68768926  0.76872284  1.          0.80097691  0.83083348\n",
      "   0.82910713  0.90267414  0.62897742  0.90447765]\n",
      " [ 0.8537235   0.6288413   0.72058348  0.80097691  1.          0.86189773\n",
      "   0.80900743  0.76811664  0.82628127  0.88186762]\n",
      " [ 0.76899953  0.61707149  0.70288038  0.83083348  0.86189773  1.\n",
      "   0.87380867  0.72371854  0.74278399  0.84600175]\n",
      " [ 0.73475192  0.61339686  0.77112718  0.82910713  0.80900743  0.87380867\n",
      "   1.          0.75534624  0.74602453  0.85221157]\n",
      " [ 0.69995341  0.54154676  0.65006077  0.90267414  0.76811664  0.72371854\n",
      "   0.75534624  1.          0.60764178  0.84144718]\n",
      " [ 0.81531999  0.63344707  0.64736141  0.62897742  0.82628127  0.74278399\n",
      "   0.74602453  0.60764178  1.          0.74742493]\n",
      " [ 0.86029135  0.73973355  0.79694558  0.90447765  0.88186762  0.84600175\n",
      "   0.85221157  0.84144718  0.74742493  1.        ]]\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#first and last pos tag freqency in each sentence\n",
    "freq_pos = TfidfVectorizer(input='filename',use_idf=False,analyzer='word',ngram_range=(1,1))\n",
    "first_pos = freq_pos.fit_transform(filenames_first).toarray()\n",
    "last_pos = freq_pos.fit_transform(filenames_last).toarray()\n",
    "#print first_pos\n",
    "#print last_pos\n",
    "\n",
    "first_posM = cosine_similarity(first_pos)\n",
    "last_posM = cosine_similarity(last_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
